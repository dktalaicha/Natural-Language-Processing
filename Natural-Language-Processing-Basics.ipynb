{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP)\n",
    "____________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is NLP? \n",
    "\n",
    "Natural language processing (NLP) is a branch of artificial intelligence that helps computers understand, interpret and manipulate human language.\n",
    "\n",
    "NLP makes it possible for computers to read text, hear speech, interpret it, measure sentiment and determine which parts are important.\n",
    "\n",
    "The input and output of an NLP system can be −\n",
    "* Speech\n",
    "* Written Text\n",
    "\n",
    "# What are Components of NLP?\n",
    "\n",
    "There are two components of NLP as given −\n",
    "\n",
    "### 1. Natural Language Understanding (NLU)\n",
    "    Understanding involves the following tasks −\n",
    "\n",
    "        * Mapping the given input in natural language into useful representations.\n",
    "        * Analyzing different aspects of the language.\n",
    "\n",
    "### 2. Natural Language Generation (NLG)\n",
    "It is the process of producing meaningful phrases and sentences in the form of natural language from some internal representation.\n",
    "\n",
    "    It involves −\n",
    "        * Text planning − It includes retrieving the relevant content from knowledge base.\n",
    "        * Sentence planning  − It includes choosing required words, forming meaningful phrases, setting tone of the sentence.\n",
    "        * Text Realization − It is mapping sentence plan into sentence structure.\n",
    "    \n",
    "\n",
    "# Difficulties in NLU\n",
    "NL has an extremely rich form and structure.\n",
    "\n",
    "It is very ambiguous. There can be different levels of ambiguity −\n",
    "\n",
    "* **Lexical ambiguity** − It is at very primitive level such as word-level.\n",
    "\n",
    "        For example, treating the word “board” as noun or verb?\n",
    "\n",
    "* **Syntax Level ambiguity** − A sentence can be parsed in different ways.\n",
    "\n",
    "        For example, “He lifted the beetle with red cap.” − Did he use cap to lift the beetle or he lifted a beetle that had red cap?\n",
    "\n",
    "* **Referential ambiguity** − Referring to something using pronouns. For example, Rima went to Gauri. She said, “I am tired.” − Exactly who is tired?\n",
    "\n",
    "One input can mean different meanings. Many inputs can mean the same thing.\n",
    "\n",
    "\n",
    "# Why is NLP important?\n",
    "* **Large volumes of textual data**\n",
    "Today’s machines can analyze more language-based data than humans, without fatigue and in a consistent, unbiased way. Considering the staggering amount of unstructured data that’s generated every day, from medical records to social media, automation will be critical to fully analyze text and speech data efficiently.\n",
    "\n",
    "* **Structuring a highly unstructured data source**\n",
    "NLP is important because it helps resolve ambiguity in language and adds useful numeric structure to the data for many downstream applications, such as speech recognition or text analytics. \n",
    "\n",
    "# How does NLP work?\n",
    "Basic NLP tasks include tokenization and parsing, lemmatization/stemming, part-of-speech tagging, language detection and identification of semantic relationships. \n",
    "\n",
    "In general terms, NLP tasks break down language into shorter, elemental pieces, try to understand relationships between the pieces and explore how the pieces work together to create meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing Library\n",
    "\n",
    "* The NLTK library in python is a massive tool kit, it stands for **Natural Language Tool Kit**.\n",
    "* It is aimed at helping you with the entire Natural Language Processing (NLP) methodology. \n",
    "* NLTK will aid you with everything from splitting sentences from paragraphs, splitting up words, recognizing the part of speech of those words, highlighting the main subjects, and then even with helping your machine to understand what the text is all about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# importing NLTK library\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK resources online installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next step is to download all the required libraries and data for nltk\n",
    "# Run below command if you want to download all the resources required by nltk using live internet connection\n",
    "# use server url: http://www.nltk.org/nltk_data/\n",
    "\n",
    "####nltk.download()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose to download \"all\" for all packages, and then click 'download.' This will give you all of the tokenizers, chunkers, other algorithms, and all of the corpora. If space is an issue, you can elect to selectively download everything manually. The NLTK module will take up about 7MB, and the entire nltk_data directory will take up about 1.8GB, which includes your chunkers, parsers, and the corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK resources offline installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/dinesh/nltk_data',\n",
       " '/home/dinesh/anaconda3/nltk_data',\n",
       " '/home/dinesh/anaconda3/share/nltk_data',\n",
       " '/home/dinesh/anaconda3/lib/nltk_data',\n",
       " '/usr/share/nltk_data',\n",
       " '/usr/local/share/nltk_data',\n",
       " '/usr/lib/nltk_data',\n",
       " '/usr/local/lib/nltk_data']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Another option is to place the already downloaded nltk_data folder at any of the below path folder\n",
    "# When we inport any module from nltk then the command searches for resources in below listed folders\n",
    "nltk.data.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/dinesh/nltk_data  ----->  False\n",
      "/home/dinesh/anaconda3/nltk_data  ----->  True\n",
      "/home/dinesh/anaconda3/share/nltk_data  ----->  False\n",
      "/home/dinesh/anaconda3/lib/nltk_data  ----->  False\n",
      "/usr/share/nltk_data  ----->  False\n",
      "/usr/local/share/nltk_data  ----->  False\n",
      "/usr/lib/nltk_data  ----->  False\n",
      "/usr/local/lib/nltk_data  ----->  False\n"
     ]
    }
   ],
   "source": [
    "# After keeping the nltk_data folder at any one of the locations we can check if this directory exists or not\n",
    "import os\n",
    "for listedPath in nltk.data.path:\n",
    "    print(listedPath, \" -----> \" ,os.path.isdir(listedPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be seen that the folder \"/home/dinesh/anaconda3/nltk_data\" exists.\n",
    "\n",
    "That means we have the required folder present!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP basic terminology\n",
    "\n",
    "Now that you have all the things that you need, let's understand some quick vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Corpus - collection of text documents\n",
    "\n",
    "A corpus can be defined as a collection of text documents. It can be thought as just a bunch of text files in a directory, often alongside many other directories of text files.\n",
    "\n",
    "Corpus is singular. Corpora is the plural of this. \n",
    "\n",
    "Example: A collection of medical journals.\n",
    "Corpus is the equivalent of \"dataset\" in a general machine learning task.\n",
    "\n",
    "## Lexicon - dictionary or  vocabulary of a language\n",
    "The definition of a lexicon is a dictionary or the vocabulary of a language, a people or a subject. \n",
    "Words and their meanings. Example: English dictionary. Consider, however, that various fields will have different lexicons. For example: To a financial investor, the first meaning for the word \"Bull\" is someone who is confident about the market, as compared to the common English lexicon, where the first meaning for the word \"Bull\" is an animal. As such, there is a special lexicon for financial investors, doctors, children, mechanics, and so on.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I would love to try or hear the sample audio your app can produce. I do not want to purchase, because I've purchased so many apps that say they do something and do not deliver.  \", \"Can you please add audio samples with text you've converted? I'd love to see the end results.\", 'Thanks!']\n"
     ]
    }
   ],
   "source": [
    "# Creating a wordlist corpus\n",
    "import nltk\n",
    "from nltk.corpus.reader import WordListCorpusReader \n",
    "x = WordListCorpusReader('.', ['datasets/sample.txt']) \n",
    "x.words() \n",
    "x.fileids() \n",
    "\n",
    "print(x.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading libraries \n",
    "import nltk.data \n",
    "  \n",
    "#nltk.data.load('corpora/indian/README', format ='raw') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('cat.n.01'),\n",
       " Synset('guy.n.01'),\n",
       " Synset('cat.n.03'),\n",
       " Synset('kat.n.01'),\n",
       " Synset('cat-o'-nine-tails.n.01'),\n",
       " Synset('caterpillar.n.02'),\n",
       " Synset('big_cat.n.01'),\n",
       " Synset('computerized_tomography.n.01'),\n",
       " Synset('cat.v.01'),\n",
       " Synset('vomit.v.01')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('cat') \n",
    "\n",
    "# synsets method Load all semantically equivalent with a given lemma and part of speech tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\n",
    "\n",
    "## Stop Words Removal\n",
    "Stop Words Removal includes getting rid of common language articles, pronouns and prepositions such as “and”, “the” or “to” in English. In this process some very common words that appear to provide little or no value to the NLP objective are filtered and excluded from the text to be processed, hence removing widespread and frequent terms that are not informative about the corresponding text.\n",
    "Stop words can be safely ignored by carrying out a lookup in a pre-defined list of keywords, freeing up database space and improving processing time. There is no universal list of stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Stopwords :  179\n",
      "\n",
      " First 20 Stopwords :  ['by', 'wouldn', 'if', 'himself', 'had', 'just', 'theirs', 'ourselves', 'into', 'yourselves', 'ma', 'hers', 'the', \"it's\", 'her', 'didn', 'shan', 'then', 'over', 'being']\n"
     ]
    }
   ],
   "source": [
    "# List of stopwords can be found in NLTK corpus API:\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = list(set(stopwords.words('english')))\n",
    "\n",
    "print(\"Number of Stopwords : \",len(stopWords))\n",
    "print(\"\\n First 20 Stopwords : \",stopWords[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith,',\n",
       " 'today?',\n",
       " 'weather',\n",
       " 'great,',\n",
       " 'Python',\n",
       " 'awesome.',\n",
       " 'sky',\n",
       " 'pinkish-blue.',\n",
       " 'eat',\n",
       " 'cardboard.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "EXAMPLE_TEXT = '''Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. \n",
    "The sky is pinkish-blue. You shouldn't eat cardboard.'''\n",
    "\n",
    "clean_mess = [word for word in EXAMPLE_TEXT.split() if word.lower() not in stopwords.words('english')]\n",
    "\n",
    "clean_mess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'today', '?', 'The', 'weather', 'great', ',', 'Python', 'awesome', '.', 'The', 'sky', 'pinkish-blue', '.', 'You', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "# Removal of stopwords\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def remove_stopword(sentence):\n",
    "    tokenize_word = word_tokenize(sentence)\n",
    "    return [w for w in tokenize_word if w not in list(set(stopwords.words('english')))]\n",
    "\n",
    "EXAMPLE_TEXT = '''Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. \n",
    "The sky is pinkish-blue. You shouldn't eat cardboard.'''\n",
    "\n",
    "print(remove_stopword(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Token \n",
    "Each \"entity\" that is a part of whatever was split up based on rules. For examples, each word is a token when a sentence is \"tokenized\" into words. Each sentence can also be a token, if you tokenized the sentences out of a paragraph.\n",
    "\n",
    "## Tokenization\n",
    "Tokenization is the process of **segmenting** running text into sentences and words. In essence, it’s the task of cutting a text into pieces called tokens, and at the same time throwing away certain characters, such as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello Mr. Smith, how are you doing today?', 'The weather is great, and Python is awesome.', 'The sky is pinkish-blue.', \"You shouldn't eat cardboard.\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "EXAMPLE_TEXT = '''Hello Mr. Smith, how are you doing today? The weather is great, and Python is awesome. \n",
    "The sky is pinkish-blue. You shouldn't eat cardboard.'''\n",
    "\n",
    "print(sent_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, you may think tokenizing by things like words or sentences is a rather trivial thing. For many sentences it can be! The first step would be likely doing a simple .split('. '), or splitting by period followed by a space. Then maybe you would bring in some regular expressions to split by period, space, and then a capital letter. The problem is that things like Mr. Smith would cause you trouble, and many other things. Splitting by word is also a challenge, especially when considering things like concatenations like we and are to we're. NLTK is going to go ahead and just save you a ton of time with this seemingly simple, yet very complex, operation.\n",
    "\n",
    "The above code will output the sentences, split up into a list of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?', 'The', 'weather', 'is', 'great', ',', 'and', 'Python', 'is', 'awesome', '.', 'The', 'sky', 'is', 'pinkish-blue', '.', 'You', 'should', \"n't\", 'eat', 'cardboard', '.']\n"
     ]
    }
   ],
   "source": [
    "# So there, we have created tokens, which are sentences. Let's tokenize by word this time:\n",
    "print(word_tokenize(EXAMPLE_TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.', 'All', 'work', 'play', 'makes', 'jack', 'dull', 'boy', '.']\n"
     ]
    }
   ],
   "source": [
    "# create a new list called wordsFiltered which contains all words which are not stop words.\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"All work and no play makes jack dull boy. All work and no play makes jack a dull boy.\"\n",
    "stopWords = set(stopwords.words('english'))\n",
    "words = word_tokenize(data)\n",
    "\n",
    "wordsFiltered = []\n",
    "\n",
    "for w in words:\n",
    "    if w not in stopWords:\n",
    "        wordsFiltered.append(w)\n",
    "\n",
    "print(wordsFiltered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming - process of slicing\n",
    "Stemming refers to the process of slicing the end or the beginning of words with the intention of removing affixes (lexical additions to the root of the word)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised Sentences : \n",
      "\n",
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence, and \\ncomputational linguistics concerned with the interactions between computers and human (natural) languages.', 'This is a very good example to explain this concept.', 'Just another line to show some good variation.'] \n",
      "\n",
      "Stemmed Sentence : \n",
      "\n",
      "['natur languag process ( nlp ) field comput scienc , artifici intellig , comput linguist concern interact comput human ( natur ) languag .', 'thi good exampl explain concept .', 'just anoth line show good variat .'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "SampleText = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
    "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
    "This is a very good example to explain this concept. Just another line to show some good variation.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(SampleText)\n",
    "print(\"Tokenised Sentences : \\n\")\n",
    "print(sentences,\"\\n\")\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "\n",
    "# Stemming\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [stemmer.stem(w) for w in words if w not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words)\n",
    "\n",
    "print(\"Stemmed Sentence : \\n\")\n",
    "print(sentences,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Lemmatization - reducing a word to its base form\n",
    "\n",
    "Lemmatization has the objective of reducing a word to its base form and grouping together different forms of the same word. \n",
    "\n",
    "Lemmatization resolves words to their dictionary form (known as lemma) for which it requires detailed dictionaries in which the algorithm can look into and link words to their corresponding lemmas.\n",
    "\n",
    "For example, the words “running”, “runs” and “ran” are all forms of the word “run”, so “run” is the lemma of all the previous words.\n",
    "\n",
    "<img src=\"images/fig-nlp-1.png\" style=\"display:none\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenised Sentences : \n",
      "\n",
      "['Natural language processing (NLP) is a field of computer science, artificial intelligence, and \\ncomputational linguistics concerned with the interactions between computers and human (natural) languages.', 'This is a very good example to explain this concept.', 'Just another line to show some good variation.'] \n",
      "\n",
      "Lemmatised Sentences : \n",
      "\n",
      "['Natural language processing ( NLP ) field computer science , artificial intelligence , computational linguistics concerned interaction computer human ( natural ) language .', 'This good example explain concept .', 'Just another line show good variation .'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "SampleText = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
    "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
    "This is a very good example to explain this concept. Just another line to show some good variation.\"\"\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(SampleText)\n",
    "print(\"Tokenised Sentences : \\n\")\n",
    "print(sentences,\"\\n\")\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatization\n",
    "for i in range(len(sentences)):\n",
    "    words = nltk.word_tokenize(sentences[i])\n",
    "    words = [lemmatizer.lemmatize(w) for w in words if w not in set(stopwords.words('english'))]\n",
    "    sentences[i] = ' '.join(words) \n",
    "    \n",
    "print(\"Lemmatised Sentences : \\n\")\n",
    "print(sentences,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words (BOW) - Frequency of words\n",
    "Bag of Words is a commonly used model that allows you to **count all words** in a piece of text. Basically it creates an **occurrence matrix** for the sentence or document, disregarding grammar and word order. These word frequencies or occurrences are then used as features for training a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of Punctution :  !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \n",
      "\n",
      "Cleaned Texts : \n",
      " ['natural language processing nlp field computer science artificial intelligence computational linguistics concerned interaction computer human natural language', 'good example explain concept', 'another line show good variation'] \n",
      "\n",
      "Bag of Words (BOW) \n",
      "\n",
      "[[0 1 1 2 0 1 0 0 1 0 1 1 1 2 0 1 2 1 1 1 0 0]\n",
      " [0 0 0 0 1 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1]]\n",
      "\n",
      "Bag of Words (BOW) Features: \n",
      "\n",
      "['another', 'artificial', 'computational', 'computer', 'concept', 'concerned', 'example', 'explain', 'field', 'good', 'human', 'intelligence', 'interaction', 'language', 'line', 'linguistics', 'natural', 'nlp', 'processing', 'science', 'show', 'variation']\n",
      "Bag of Words (BOW) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>artificial</th>\n",
       "      <th>computational</th>\n",
       "      <th>computer</th>\n",
       "      <th>concept</th>\n",
       "      <th>concerned</th>\n",
       "      <th>example</th>\n",
       "      <th>explain</th>\n",
       "      <th>field</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>line</th>\n",
       "      <th>linguistics</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "      <th>science</th>\n",
       "      <th>show</th>\n",
       "      <th>variation</th>\n",
       "      <th>originalText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>natural language processing nlp field computer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>good example explain concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>another line show good variation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   another  artificial  computational  computer  concept  concerned  example  \\\n",
       "0        0           1              1         2        0          1        0   \n",
       "1        0           0              0         0        1          0        1   \n",
       "2        1           0              0         0        0          0        0   \n",
       "\n",
       "   explain  field  good  ...  language  line  linguistics  natural  nlp  \\\n",
       "0        0      1     0  ...         2     0            1        2    1   \n",
       "1        1      0     1  ...         0     0            0        0    0   \n",
       "2        0      0     1  ...         0     1            0        0    0   \n",
       "\n",
       "   processing  science  show  variation  \\\n",
       "0           1        1     0          0   \n",
       "1           0        0     0          0   \n",
       "2           0        0     1          1   \n",
       "\n",
       "                                        originalText  \n",
       "0  natural language processing nlp field computer...  \n",
       "1                       good example explain concept  \n",
       "2                   another line show good variation  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bag of Words (BOW)\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "SampleText = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
    "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
    "This is a very good example to explain this concept. Just another line to show some good variation.\"\"\"\n",
    "\n",
    "# Cleaning the texts\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(SampleText)\n",
    "\n",
    "print(\"List of Punctution : \", string.punctuation, \"\\n\")\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    cleanText = [char for char in sentences[i] if char not in string.punctuation]\n",
    "    cleanText = ''.join(cleanText)\n",
    "    cleanText = cleanText.lower()\n",
    "    cleanText = cleanText.split()\n",
    "    cleanText = [wordnet.lemmatize(w) for w in cleanText if not w in set(stopwords.words('english'))]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "    corpus.append(cleanText)\n",
    "    \n",
    "print(\"Cleaned Texts : \\n\",corpus,\"\\n\")\n",
    "\n",
    "# Creating the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer(max_features = 1500)\n",
    "bow = cv.fit_transform(corpus)\n",
    "\n",
    "print(\"Bag of Words (BOW) \\n\")\n",
    "print(bow.toarray())\n",
    "\n",
    "print(\"\\nBag of Words (BOW) Features: \\n\")\n",
    "print(cv.get_feature_names())\n",
    "\n",
    "\n",
    "# Visualizing the Document Term Matrix\n",
    "print(\"Bag of Words (BOW) \\n\")\n",
    "VectorizedText = pd.DataFrame(bow.toarray(), columns=cv.get_feature_names())\n",
    "VectorizedText['originalText'] = pd.Series(corpus)\n",
    "VectorizedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Words to Vector\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "SampleText = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
    "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
    "This is a very good example to explain this concept. Just another line to show some good variation.\"\"\"\n",
    "\n",
    "# Cleaning the texts\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(SampleText)\n",
    "words = [nltk.word_tokenize(s) for s in sentences]\n",
    "\n",
    "print(\"List of Punctution : \", string.punctuation, \"\\n\")\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(words)):\n",
    "    cleanText = [char for char in words[i] if char not in string.punctuation]\n",
    "    cleanText = ''.join(cleanText)\n",
    "    cleanText = cleanText.lower()\n",
    "    cleanText = cleanText.split()\n",
    "    cleanText = [wordnet.lemmatize(w) for w in cleanText if not w in set(stopwords.words('english'))]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "    corpus.append(cleanText)\n",
    "    \n",
    "print(\"Cleaned Texts : \\n\",corpus,\"\\n\")\n",
    "\n",
    "# Training the Word2Vec model\n",
    "model = Word2Vec(corpus, min_count=1)\n",
    "\n",
    "\n",
    "words = model.wv.vocab\n",
    "\n",
    "# Finding Word Vectors\n",
    "vector = model.wv['war']\n",
    "\n",
    "# Most similar words\n",
    "similar = model.wv.most_similar('vikram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling - uncovering hidden structures\n",
    "Topic Modeling is as a **method for uncovering hidden structures** in sets of texts or documents. In essence it clusters texts to discover latent topics based on their contents, processing individual words and assigning them values based on their distribution. This technique is based on the assumptions that each document consists of a mixture of topics and that each topic consists of a set of words, which means that if we can spot these hidden topics we can unlock the meaning of our texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams - combination of N words\n",
    "A combination of N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others. The following code generates bigram of a text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ngrams(text, WordsToCombine):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)- WordsToCombine+1):\n",
    "        output.append(words[i:i+WordsToCombine])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'],\n",
       " ['is', 'a'],\n",
       " ['a', 'very'],\n",
       " ['very', 'good'],\n",
       " ['good', 'book'],\n",
       " ['book', 'to'],\n",
       " ['to', 'study']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(text='this is a very good book to study', WordsToCombine=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('this', 'is', 'a')\n",
      "('is', 'a', 'very')\n",
      "('a', 'very', 'good')\n",
      "('very', 'good', 'book')\n",
      "('good', 'book', 'to')\n",
      "('book', 'to', 'study')\n"
     ]
    }
   ],
   "source": [
    "# NLTK function to generate ngrams\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "samplText='this is a very good book to study'\n",
    "NGRAMS=ngrams(sequence=nltk.word_tokenize(samplText), n=3)\n",
    "for grams in NGRAMS:\n",
    "    print(grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TextBlob\n",
    "\n",
    "TextBlob is a Python library for processing textual data. It provides a consistent API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problem Statement: \n",
    "I have a long text (review/tweets/news), I need to understand what subjects are being talked and what is the sentiment of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
      "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
      "This is a very good example to explain this concept. Just another line to show some good variation.\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "SampleText = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
    "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
    "This is a very good example to explain this concept. Just another line to show some good variation.\"\"\"\n",
    "\n",
    "# Converting the sample text to a blob\n",
    "SampleTextInBlobFormat = TextBlob(SampleText)\n",
    "\n",
    "print(SampleTextInBlobFormat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis\n",
    "https://towardsdatascience.com/basic-binary-sentiment-analysis-using-nltk-c94ba17ae386"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.20166666666666666, subjectivity=0.5466666666666667)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentiment=-1 means 100% negative sentiment\n",
    "# Sentiment=+1 means 100% positive sentiment\n",
    "# Sentiment=0 means no sentiment\n",
    "\n",
    "# Subjectivity =1 means the text is meaningful.\n",
    "# Subjectivity =0 means the text does not has any meaning.\n",
    "\n",
    "SampleTextInBlobFormat.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Phrases Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList(['natural language processing', 'nlp', 'computer science', 'artificial intelligence', 'computational linguistics', 'good example', 'good variation'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Finding the noun phrases (important keywords combination) in the text\n",
    "# This can help to find out what entities are being talked about in the given text\n",
    "NounPhrases=SampleTextInBlobFormat.noun_phrases\n",
    "NounPhrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud\n",
    "Plotting wordcloud of the noun phrases can help visualize what are the topics/entities being discussed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural_language_processing',\n",
       " 'nlp',\n",
       " 'computer_science',\n",
       " 'artificial_intelligence',\n",
       " 'computational_linguistics',\n",
       " 'good_example',\n",
       " 'good_variation']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating an empty list to hold new values\n",
    "# combining the noun phrases using underscore to visualize it as wordcloud\n",
    "NewNounList=[]\n",
    "for words in NounPhrases:\n",
    "    NewNounList.append(words.replace(\" \", \"_\"))\n",
    "\n",
    "NewNounList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural_language_processing nlp computer_science artificial_intelligence computational_linguistics good_example good_variation'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting list into a string to plot wordcloud\n",
    "NewNounString = ' '.join(NewNounList)\n",
    "NewNounString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "customStopwords=list(STOPWORDS)\n",
    "wordcloudimage = WordCloud(max_words=50,\n",
    "                           font_step=2 ,\n",
    "                            max_font_size=500,\n",
    "                            stopwords=customStopwords + ['nlp'],\n",
    "                            width=2000,\n",
    "                            height=2000\n",
    "                          ).generate(NewNounString)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wordcloudimage)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tagging\n",
    "One of the more powerful aspects of the NLTK module is the **Part of Speech** tagging that it can do for you. This means labeling words in a sentence as nouns, adjectives, verbs...etc. Even more impressive, it also labels by tense, and more. Here's a list of the tags, what they mean, and some examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**POS tag list:**\n",
    "\n",
    "*  CC----->coordinating conjunction\n",
    "*  CD----->cardinal digit\n",
    "*  DT----->determiner\n",
    "*  EX----->existential there (like: \"there is\" ... think of it like \"there exists\")\n",
    "*  FW----->foreign word\n",
    "*  IN----->preposition/subordinating conjunction\n",
    "*  JJ----->adjective\t'big'\n",
    "*  JJR---->adjective, comparative\t'bigger'\n",
    "*  JJS---->adjective, superlative\t'biggest'\n",
    "*  LS----->list marker\t1)\n",
    "*  MD----->modal\tcould, will\n",
    "*  NN----->noun, singular 'desk'\n",
    "*  NNS---->noun plural\t'desks'\n",
    "*  NNP---->proper noun, singular\t'Harrison'\n",
    "*  NNPS--->proper noun, plural\t'Americans'\n",
    "*  PDT---->predeterminer\t'all the kids'\n",
    "*  POS---->possessive ending\tparent's\n",
    "*  PRP---->personal pronoun\tI, he, she\n",
    "*  PRP---->possessive pronoun\tmy, his, hers\n",
    "*  RB----->adverb\tvery, silently,\n",
    "*  RBR---->adverb, comparative\tbetter\n",
    "*  RBS---->adverb, superlative\tbest\n",
    "*  RP----->particle\tgive up\n",
    "*  TO----->to\tgo 'to' the store.\n",
    "*  UH----->interjection\n",
    "*  VB----->verb, base form\ttake\n",
    "*  VBD---->verb, past tense\ttook\n",
    "*  VBG---->verb, gerund/present participle\ttaking\n",
    "*  VBN---->verb, past participle\ttaken\n",
    "*  VBP---->verb, sing. present, non-3d\ttake\n",
    "*  VBZ---->verb, 3rd person sing. present\ttakes\n",
    "*  WDT---->wh-determiner\twhich\n",
    "*  WP----->wh-pronoun\twho, what\n",
    "*  WP----->possessive wh-pronoun\twhose\n",
    "*  WRB---->wh-abverb\twhere, when"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of below code snippet will be a list of tuples, where the first element in the tuple is the word, and the second is the part of speech tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('very', 'RB'),\n",
       " ('good', 'JJ'),\n",
       " ('book', 'NN')]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "# Take a quick look into the POS tagger from nltk\n",
    "# The function will identify what part of speech is represented by each word\n",
    "nltk.pos_tag(word_tokenize('This is a very good book'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking - group into noun phrases\n",
    "\n",
    "We know that the parts of speech like noun, verb, adverb, prefix etc.  We can do what is called chunking, and group words into hopefully meaningful chunks. One of the main goals of chunking is to group into what are known as \"noun phrases.\" These are phrases of one or more words that contain a noun, maybe some descriptive words, maybe a verb, and maybe something like an adverb. The idea is to group nouns with the words that are in relation to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nouns:  book\n",
      "Adjectives:  good nice\n",
      "Overall sentiment score of ajdectives:  Sentiment(polarity=0.6499999999999999, subjectivity=0.8)\n"
     ]
    }
   ],
   "source": [
    "# Finding only the individual Nouns and adjective chunks\n",
    "import nltk\n",
    "sampleSentence = '''This is a very good book, felt really nice reading it.'''\n",
    "#sampleSentence = '''Ravi is a bad leader, he misguides the team too often'''\n",
    "\n",
    "sampleSentence = nltk.word_tokenize(sampleSentence)\n",
    "sampleSentencePOS = nltk.pos_tag(sampleSentence)\n",
    "\n",
    "#Find Nouns or proper Nouns\n",
    "OnlyNouns = (\" \").join([POStags[0] for POStags in sampleSentencePOS if POStags[1] in ['NN','NNP']])\n",
    "\n",
    "# Find only Adjectives\n",
    "OnlyAdjectives= (\" \").join([POStags[0] for POStags in sampleSentencePOS if POStags[1] in ['JJ','JJR','JJS']])\n",
    "\n",
    "print ('Nouns: ', OnlyNouns)\n",
    "print ('Adjectives: ', OnlyAdjectives)\n",
    "print('Overall sentiment score of ajdectives: ',TextBlob(OnlyAdjectives).sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can give us idea about what type of adjectives are being used for the nouns, whether they are positive or negative keywords\n",
    "\n",
    "A Wordcloud can be plotted for all the adjectives to understand overall sentiment or for all the nouns to understand what are the nouns being talked about\n",
    "\n",
    "Sentiment score can be generated by checking if all the adjectives used are positive or negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking multiple grammer rules "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(NP very/RB good/JJ book/NN)\n",
      "(NP nice/JJ reading/VBG)\n"
     ]
    }
   ],
   "source": [
    "# Generating grammer rules to find such sentences in data\n",
    "PatternsToFind = '''NP: {<JJ><VBG>} \n",
    "                    NP: {<RB><JJ><NN>} '''\n",
    "\n",
    "sampleSentence = '''This is a very good book, felt really nice reading it.'''\n",
    "#sampleSentence = '''Ravi is a bad leader, he misguides the team too often'''\n",
    "\n",
    "# Creating Parts of Speech(POS) tags\n",
    "sampleSentence = nltk.word_tokenize(sampleSentence)\n",
    "sampleSentencePOS = nltk.pos_tag(sampleSentence)\n",
    "\n",
    "# Chunking the listed patterns\n",
    "PatternParser = nltk.RegexpParser(PatternsToFind)\n",
    "ParsedResults = PatternParser.parse(sampleSentencePOS)\n",
    "\n",
    "# Getting the pattern results from the text data\n",
    "for results in ParsedResults:\n",
    "    #print(results,'--' ,type(results))\n",
    "    \n",
    "    # Printing only the extracted patterns\n",
    "    if(type(results)==nltk.tree.Tree):\n",
    "        print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most major forms of chunking in natural language processing is called \"Named Entity Recognition.\" The idea is to have the machine immediately be able to pull out \"entities\" like people, places, things, locations, monetary figures, and more.\n",
    "\n",
    "This can be a bit of a challenge, but the library Spacy has this built in for us.\n",
    "\n",
    "Named Entity Type and Examples\n",
    " * ORGANIZATION - Georgia-Pacific Corp., WHO\n",
    " * PERSON - Eddy Bonte, President Obama\n",
    " * LOCATION - Murray River, Mount Everest\n",
    " * DATE - June, 2008-06-29\n",
    " * TIME - two fifty a m, 1:30 p.m.\n",
    " * MONEY - 175 million Canadian Dollars, GBP 10.40\n",
    " * PERCENT - twenty pct, 18.75 %\n",
    " * FACILITY - Washington Monument, Stonehenge\n",
    " * GPE - South East Asia, Midlothian\n",
    " * NORP- Nationality OR Religious or Political groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy \n",
    "Named Entity Recognition (NER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing the required libraries for spacy\n",
    "spaCy is a free, open-source library for advanced Natural Language Processing (NLP) in Python. It helps to perform NLP using pre-trained models. we are going to use one such model called \"en_core_web_sm\"\n",
    "\n",
    "More information at : https://spacy.io/models/en#en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Indian ministry', 'ORG')]\n",
      "[('$1 billion', 'MONEY')]\n",
      "[('Wednesday', 'DATE')]\n",
      "[('Sitaraman', 'PERSON')]\n",
      "[('Delhi', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "SampleSentence = '''Indian ministry collected record $1 billion as taxes on Wednesday, finance minister Sitaraman announced in Delhi'''\n",
    "#SampleSentence ='''we will go to Delhi to meet the director of the VCorp, Mr. Ramesh'''\n",
    "\n",
    "FoundEntities=nlp(SampleSentence)\n",
    "for X in FoundEntities.ents:\n",
    "    print([(X.text, X.label_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a custom NER model\n",
    "Manually build a NER to recognize a given type of word by showing a lot of examples to the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "\n",
    "# Creating a sample data to train the NER model which idetifies phone numbers\n",
    "# For high accuracy, large amount of training data is required (at least 500 examples)\n",
    "\n",
    "TRAIN_DATA = [\n",
    "        (\"plese note my phone number is 9923412334\", {\"entities\": [(30, 40, \"PHONE\")]}),\n",
    "        (\"9344528345 is the support phone phone number\", {\"entities\": [(0, 10, \"PHONE\")]}),\n",
    "        (\"9344225345 my phone number is on that list\", {\"entities\": [(0, 10, \"PHONE\")]}),\n",
    "        (\"can you tell if the phone number is 8344528345\", {\"entities\": [(36, 46, \"PHONE\")]}),\n",
    "        (\"can you note my number? it is 8324528345\", {\"entities\": [(30, 40, \"PHONE\")]}),\n",
    "        (\"it is 9344528345\", {\"entities\": [(6, 16, \"PHONE\")]}),\n",
    "        (\"phone number is 8944528341\", {\"entities\": [(16, 26, \"PHONE\")]})\n",
    "]\n",
    "\n",
    "\n",
    "# Creating a blank spacy NER model\n",
    "custom_NER = spacy.blank(\"en\")\n",
    "\n",
    "# give a name to our list of vectors\n",
    "custom_NER.vocab.vectors.name = 'example_model_training'\n",
    "\n",
    "# our pipeline would only do NER\n",
    "ner = custom_NER.create_pipe('ner')\n",
    "\n",
    "# we add the pipeline to the model\n",
    "custom_NER.add_pipe(ner, last=True) \n",
    "\n",
    "# Letting the model know about the entitiy we are trying to train\n",
    "custom_NER.entity.add_label('PHONE')\n",
    "\n",
    "optimizer = custom_NER.begin_training()\n",
    "\n",
    "# Training the model in 10 iterations by randomly shuffling the input\n",
    "for i in range(10):\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    for text, annotations in TRAIN_DATA:\n",
    "        custom_NER.update([text], [annotations], sgd=optimizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('7774448832', 'PHONE')]\n"
     ]
    }
   ],
   "source": [
    "# Calling the new custom NER model\n",
    "FoundEntities=custom_NER('please save my number it is 7774448832')\n",
    "for X in FoundEntities.ents:\n",
    "    print([(X.text, X.label_)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting unstructured text data into structured features\n",
    "\n",
    "\n",
    "## Text Pre-processing\n",
    "\n",
    "Our main issue with our data is that it is all in text format (strings). The classification algorithms that we've learned about so far will need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the the [bag-of-words](http://en.wikipedia.org/wiki/Bag-of-words_model) approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "\n",
    "In this section we'll convert the raw messages (sequence of characters) into vectors (sequences of numbers).\n",
    "\n",
    "As a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here.\n",
    "\n",
    "Let's create a function that will process the string in the message column, then we can just use **apply()** in pandas do process all the text in the DataFrame.\n",
    "\n",
    "First removing punctuation. We can just take advantage of Python's built-in **string** library to get a quick list of all the possible punctuation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Term matrix using word Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'book', 'experience', 'feeling', 'happy', 'hotel', 'mobile', 'new', 'nice', 'phone', 'read', 'real']\n",
      "(3, 12)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>book</th>\n",
       "      <th>experience</th>\n",
       "      <th>feeling</th>\n",
       "      <th>happy</th>\n",
       "      <th>hotel</th>\n",
       "      <th>mobile</th>\n",
       "      <th>new</th>\n",
       "      <th>nice</th>\n",
       "      <th>phone</th>\n",
       "      <th>read</th>\n",
       "      <th>real</th>\n",
       "      <th>originalText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>This book is nice to read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>I had real bad experience in that hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Feeling happy about the new mobile phone!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad  book  experience  feeling  happy  hotel  mobile  new  nice  phone  \\\n",
       "0    0     1           0        0      0      0       0    0     1      0   \n",
       "1    1     0           1        0      0      1       0    0     0      0   \n",
       "2    0     0           0        1      1      0       1    1     0      1   \n",
       "\n",
       "   read  real                               originalText  \n",
       "0     1     0                  This book is nice to read  \n",
       "1     0     1    I had real bad experience in that hotel  \n",
       "2     0     0  Feeling happy about the new mobile phone!  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count vectorization of text\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "corpus = [\n",
    "    'This book is nice to read',\n",
    "    'I had real bad experience in that hotel',\n",
    "    'Feeling happy about the new mobile phone!']\n",
    "\n",
    "#vectorizer = CountVectorizer(stop_words=None)\n",
    "vectorizer = CountVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)\n",
    "\n",
    "# Visualizing the Document Term Matrix\n",
    "import pandas as pd\n",
    "VectorizedText=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "VectorizedText['originalText']=pd.Series(corpus)\n",
    "VectorizedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF\n",
    "\n",
    "TF-IDF stands for *term frequency-inverse document frequency*, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    "**TF: Term Frequency**, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$ TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}. $$\n",
    "</div>    \n",
    "\n",
    "**IDF: Inverse Document Frequency**, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: \n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "$$ IDF(t) = \\log_e\\Bigg( \\frac{\\text{Total number of documents}}{\\text{Number of documents with term t in it}} \\Bigg). $$\n",
    "</div>\n",
    "\n",
    "See below for a simple example.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. \n",
    "\n",
    "The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "\n",
    "\n",
    "\n",
    "### Document Term matrix using TF-IDF scores\n",
    "* TF-IDF is a composite score representing the power of a given word to uniquely identify the document\n",
    "* It is computed by multiplying Term Frequency(TF) and Inverse Document Frequency(IDF)\n",
    "\n",
    "\n",
    "* IF a word is very common, then IDF is near to zero, otherwise its close to 1\n",
    "* The higher the tf-idf value of a word, the more unique/rare occuring that word is.\n",
    "* If the tf-idf is close to zero, it means the word is very commonly used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bad', 'book', 'experience', 'feeling', 'happy', 'hotel', 'mobile', 'new', 'nice', 'phone', 'read', 'real']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bad</th>\n",
       "      <th>book</th>\n",
       "      <th>experience</th>\n",
       "      <th>feeling</th>\n",
       "      <th>happy</th>\n",
       "      <th>hotel</th>\n",
       "      <th>mobile</th>\n",
       "      <th>new</th>\n",
       "      <th>nice</th>\n",
       "      <th>phone</th>\n",
       "      <th>read</th>\n",
       "      <th>real</th>\n",
       "      <th>originalText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.0</td>\n",
       "      <td>This book is nice to read</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>I had real bad experience in that hotel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Feeling happy about the new mobile phone!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bad     book  experience   feeling     happy  hotel    mobile       new  \\\n",
       "0  0.0  0.57735         0.0  0.000000  0.000000    0.0  0.000000  0.000000   \n",
       "1  0.5  0.00000         0.5  0.000000  0.000000    0.5  0.000000  0.000000   \n",
       "2  0.0  0.00000         0.0  0.447214  0.447214    0.0  0.447214  0.447214   \n",
       "\n",
       "      nice     phone     read  real                               originalText  \n",
       "0  0.57735  0.000000  0.57735   0.0                  This book is nice to read  \n",
       "1  0.00000  0.000000  0.00000   0.5    I had real bad experience in that hotel  \n",
       "2  0.00000  0.447214  0.00000   0.0  Feeling happy about the new mobile phone!  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF vectorization of text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "corpus = [\n",
    "    'This book is nice to read',\n",
    "    'I had real bad experience in that hotel',\n",
    "    'Feeling happy about the new mobile phone!']\n",
    "\n",
    "#vectorizer = TfidfVectorizer(stop_words=None)\n",
    "vectorizer = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS)\n",
    "\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "\n",
    "# Visualizing the Document Term Matrix using TF-IDF\n",
    "VectorizedText=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "VectorizedText['originalText']=pd.Series(corpus)\n",
    "VectorizedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Texts : \n",
      " ['natural language processing nlp field computer science artificial intelligence computational linguistics concerned interaction computer human natural language', 'good example explain concept', 'another line show good variation'] \n",
      "\n",
      "Cleaned feature names : \n",
      "\n",
      "['another', 'artificial', 'computational', 'computer', 'concept', 'concerned', 'example', 'explain', 'field', 'good', 'human', 'intelligence', 'interaction', 'language', 'line', 'linguistics', 'natural', 'nlp', 'processing', 'science', 'show', 'variation']\n",
      "Shape :  (3, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>another</th>\n",
       "      <th>artificial</th>\n",
       "      <th>computational</th>\n",
       "      <th>computer</th>\n",
       "      <th>concept</th>\n",
       "      <th>concerned</th>\n",
       "      <th>example</th>\n",
       "      <th>explain</th>\n",
       "      <th>field</th>\n",
       "      <th>good</th>\n",
       "      <th>...</th>\n",
       "      <th>language</th>\n",
       "      <th>line</th>\n",
       "      <th>linguistics</th>\n",
       "      <th>natural</th>\n",
       "      <th>nlp</th>\n",
       "      <th>processing</th>\n",
       "      <th>science</th>\n",
       "      <th>show</th>\n",
       "      <th>variation</th>\n",
       "      <th>originalText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.417029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.417029</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.208514</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>natural language processing nlp field computer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.528635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.402040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>good example explain concept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.355432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>0.467351</td>\n",
       "      <td>another line show good variation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    another  artificial  computational  computer   concept  concerned  \\\n",
       "0  0.000000    0.208514       0.208514  0.417029  0.000000   0.208514   \n",
       "1  0.000000    0.000000       0.000000  0.000000  0.528635   0.000000   \n",
       "2  0.467351    0.000000       0.000000  0.000000  0.000000   0.000000   \n",
       "\n",
       "    example   explain     field      good  ...  language      line  \\\n",
       "0  0.000000  0.000000  0.208514  0.000000  ...  0.417029  0.000000   \n",
       "1  0.528635  0.528635  0.000000  0.402040  ...  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.355432  ...  0.000000  0.467351   \n",
       "\n",
       "   linguistics   natural       nlp  processing   science      show  variation  \\\n",
       "0     0.208514  0.417029  0.208514    0.208514  0.208514  0.000000   0.000000   \n",
       "1     0.000000  0.000000  0.000000    0.000000  0.000000  0.000000   0.000000   \n",
       "2     0.000000  0.000000  0.000000    0.000000  0.000000  0.467351   0.467351   \n",
       "\n",
       "                                        originalText  \n",
       "0  natural language processing nlp field computer...  \n",
       "1                       good example explain concept  \n",
       "2                   another line show good variation  \n",
       "\n",
       "[3 rows x 23 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TF-IDF model\n",
    "\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "SampleText = \"\"\"Natural language processing (NLP) is a field of computer science, artificial intelligence, and \n",
    "computational linguistics concerned with the interactions between computers and human (natural) languages. \n",
    "This is a very good example to explain this concept. Just another line to show some good variation.\"\"\"\n",
    "\n",
    "# Cleaning the texts\n",
    "ps = PorterStemmer()\n",
    "wordnet=WordNetLemmatizer()\n",
    "sentences = nltk.sent_tokenize(SampleText)\n",
    "\n",
    "corpus = []\n",
    "for i in range(len(sentences)):\n",
    "    cleanText = [char for char in sentences[i] if char not in string.punctuation]\n",
    "    cleanText = ''.join(cleanText)\n",
    "    cleanText = cleanText.lower()\n",
    "    cleanText = cleanText.split()\n",
    "    cleanText = [wordnet.lemmatize(w) for w in cleanText if not w in set(stopwords.words('english'))]\n",
    "    cleanText = ' '.join(cleanText)\n",
    "    corpus.append(cleanText)\n",
    "    \n",
    "print(\"Cleaned Texts : \\n\",corpus,\"\\n\")\n",
    "\n",
    "# Creating the TF-IDF model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(\"Cleaned feature names : \\n\")\n",
    "print(vectorizer.get_feature_names())\n",
    "print(\"Shape : \",X.shape)\n",
    "\n",
    "# Visualizing the Document Term Matrix using TF-IDF\n",
    "VectorizedText=pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
    "VectorizedText['originalText']=pd.Series(corpus)\n",
    "VectorizedText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization\n",
    "\n",
    "After text preprocessing, we left with lists of tokens (also known as [lemmas](http://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html)) and now we need to convert each of those tokens into a vector the SciKit Learn's algorithm models can work with.\n",
    "\n",
    "Three steps to convert list of tokens (lemmas) into a vector that machine learning models can understand:\n",
    "\n",
    "1. Count how many times does a word occur in each sentence (Known as term frequency)\n",
    "\n",
    "2. Weigh the counts, so that frequent tokens get lower weight (inverse document frequency)\n",
    "\n",
    "3. Normalize the vectors to unit length, to abstract from the original text length (L2 norm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What to do with vectorized text?\n",
    "* This data can further be used in machine learning\n",
    "* If the text data also as target variable e.g. sentiment(positive/negative) or Support Ticket Priority (P1/P2/P3) then these word columns act as predictors and we can fit a classification/regression ML algorithm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
